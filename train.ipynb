{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798236d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import music21 as m21\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "SINGLE_FILE_DATASET = \"/content/generating-melodies-with-rnn-lstm/file_dataset\"\n",
    "MAPPING_PATH = \"/content/generating-melodies-with-rnn-lstm/mapping.json\"\n",
    "SEQUENCE_LENGTH=64\n",
    "def load(file_path):\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        song = fp.read()\n",
    "    return song\n",
    "def convert_songs_to_int(songs):\n",
    "    int_songs = []\n",
    "\n",
    "    # load mappings\n",
    "    with open(MAPPING_PATH, \"r\") as fp:\n",
    "        mappings = json.load(fp)\n",
    "\n",
    "    # transform songs string to list\n",
    "    songs = songs.split()\n",
    "\n",
    "    # map songs to int\n",
    "    for symbol in songs:\n",
    "        int_songs.append(mappings[symbol])\n",
    "\n",
    "    return int_songs\n",
    "\n",
    "\n",
    "def generate_training_sequences(sequence_length):\n",
    "    \"\"\"Create input and output data samples for training. Each sample is a sequence.\n",
    "\n",
    "    :param sequence_length (int): Length of each sequence. With a quantisation at 16th notes, 64 notes equates to 4 bars\n",
    "\n",
    "    :return inputs (ndarray): Training inputs\n",
    "    :return targets (ndarray): Training targets\n",
    "    \"\"\"\n",
    "\n",
    "    # load songs and map them to int\n",
    "    songs = load(SINGLE_FILE_DATASET)\n",
    "    int_songs = convert_songs_to_int(songs)\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    # generate the training sequences\n",
    "    num_sequences = len(int_songs) - sequence_length\n",
    "    for i in range(num_sequences):\n",
    "        inputs.append(int_songs[i:i+sequence_length])\n",
    "        targets.append(int_songs[i+sequence_length])\n",
    "\n",
    "    # one-hot encode the sequences\n",
    "    vocabulary_size = len(set(int_songs))\n",
    "    # inputs size: (# of sequences, sequence length, vocabulary size)\n",
    "    inputs = keras.utils.to_categorical(inputs, num_classes=vocabulary_size)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    print(f\"There are {len(inputs)} sequences.\")\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "OUTPUT_UNITS = 38\n",
    "NUM_UNITS = [256]\n",
    "LOSS = \"sparse_categorical_crossentropy\"\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 90\n",
    "BATCH_SIZE = 64\n",
    "SAVE_MODEL_PATH = \"model.h5\"\n",
    "\n",
    "\n",
    "def build_model(output_units, num_units, loss, learning_rate):\n",
    "    \"\"\"Builds and compiles model\n",
    "\n",
    "    :param output_units (int): Num output units\n",
    "    :param num_units (list of int): Num of units in hidden layers\n",
    "    :param loss (str): Type of loss function to use\n",
    "    :param learning_rate (float): Learning rate to apply\n",
    "\n",
    "    :return model (tf model): Where the magic happens :D\n",
    "    \"\"\"\n",
    "\n",
    "    # create the model architecture\n",
    "    input = keras.layers.Input(shape=(None, output_units))\n",
    "    x = keras.layers.LSTM(num_units[0])(input)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    output = keras.layers.Dense(output_units, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(input, output)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(output_units=OUTPUT_UNITS, num_units=NUM_UNITS, loss=LOSS, learning_rate=LEARNING_RATE):\n",
    "    \"\"\"Train and save TF model.\n",
    "\n",
    "    :param output_units (int): Num output units\n",
    "    :param num_units (list of int): Num of units in hidden layers\n",
    "    :param loss (str): Type of loss function to use\n",
    "    :param learning_rate (float): Learning rate to apply\n",
    "    \"\"\"\n",
    "\n",
    "    # generate the training sequences\n",
    "    inputs, targets = generate_training_sequences(SEQUENCE_LENGTH)\n",
    "\n",
    "    # build the network\n",
    "    model = build_model(output_units, num_units, loss, learning_rate)\n",
    "\n",
    "    # train the model\n",
    "    model.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # save the model\n",
    "    model.save(SAVE_MODEL_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
